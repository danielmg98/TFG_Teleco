la
tesis
va
sobre
clasificación
de
páginas
web
en
dominios
específicos
.
ha
sido
dirigida
como
digo
por
el
doctor
don
Anselmo
Peñas
y
en
primer
lugar
hablar
de
qué
significa
esto
de
de
clasificación
en
estos
dominios
específicos
.
en
Internet
tenemos
mucha
información
la
primera
clasificación
de
esta
información
podría
ser
en
dominios
genéricos
como
páginas
sobre
academias
páginas
académicas
información
académica
páginas
de
blog
páginas
corporativas
de
empresas
de
información
de
administraciones
públicas
de
entretenimiento
personales
tiendas
etcétera
.
dominios
específicos
sería
entrar
en
en
un
grano
más
fino
de
de
clasificación
sobre
este
sobre
toda
esta
información
.
sobre
uno
de
ellos
por
ejemplo
entretenimiento
tendríamos
un
dominio
específico
que
sería
el
teatro
.
dentro
de
teatro
existen
una
serie
de
categorías
revistas
compañías
festivales
salas
que
hablan
de
teatro
.
entonces
el
marco
de
la
tesis
se
ha
se
ha
se
ha
centrado
en
esto
en
en
obtener
una
clasificación
dentro
de
de
de
estos
de
este
dominio
específico
del
teatro
no
.
podría
ser
aplicable
es
una
de
las
líneas
de
investigación
que
se
deberá
seguir
aplicable
a
otros
dominios
pero
siempre
dentro
de
un
marco
específico
.
el
porqué
a
lo
largo
de
la
tesis
lo
veremos
%eh
pero
adelantar
un
poco
que
es
básicamente
son
las
líneas
de
futuro
.
%eh
los
buscadores
cada
vez
más
pretenden
dar
información
que
sea
útil
al
usuario
y
para
que
sea
útil
tiene
que
ser
de
grano
fino
.
no
puedes
decirle
a
una
persona
que
una
página
es
de
teatro
o
es
de
o
es
de
de
una
universidad
.
eso
ya
lo
sabe
tienes
que
darle
información
que
él
esté
buscando
vale
.
estructuramos
la
la
presentación
y
la
investigación
en
tres
grandes
puntos
definiendo
primero
el
problema
y
los
objetivos
que
se
han
seguido
para
intentar
darle
solución
los
experimentos
que
se
han
realizado
para
guiar
esta
esta
investigación
y
las
conclusiones
a
las
que
se
han
llegado
y
las
aplicaciones
y
líneas
de
investigación
futuras
que
requeriría
.
la
web
es
es
muy
grande
.
es
el
mayor
repositor
de
información
en
la
actualidad
es
una
información
muy
dinámica
eh
se
utiliza
para
consultar
hoy
será
de
lo
que
más
se
utiliza
para
para
realizar
consultas
por
parte
del
usuario
para
obtener
información
válida
a
partir
de
esta
maraña
de
información
se
necesita
dar
orden
.
como
es
tan
grande
y
es
tan
dinámica
no
se
puede
dar
orden
de
manera
manual
por
lo
que
se
necesita
métodos
de
clasificación
automática
.
por
otro
lado
otro
problema
que
está
surgiendo
con
el
acercamiento
tecnológico
a
la
a
la
población
es
que
cada
vez
más
hay
grupos
sociales
que
deben
de
estar
protegidos
.
el
ejemplo
más
directo
es
el
de
los
niños
que
están
en
un
colegio
empiezan
sus
clases
de
informática
tienen
un
ordenador
con
conexión
a
Internet
y
no
deberían
de
poder
ver
contenidos
perniciosos
pues
de
de
pornografía
de
violencia
o
o
de
juegos
.
además
la
web
está
evolucionando
la
web
está
muy
de
moda
está
marcando
las
tendencias
de
futuro
toda
la
colaboración
entre
usuarios
para
ir
creando
servicios
avanzados
y
cómo
no
los
blogs
los
blogs
están
teniendo
una
auge
bestial
todo
el
mundo
prácticamente
todo
el
mundo
ya
tiene
un
blog
hoy
en
día
esto
se
traduce
en
necesidad
de
buscadores
específicos
de
esta
información
.
Google
tiene
un
buscador
de
blogs
que
no
es
realmente
un
buscador
de
blogs
porque
te
encuentra
todas
aquellas
páginas
que
tienen
una
suscripción
una
suscripción
RSS
ATOM
de
este
estilo
que
tengan
que
ver
con
tu
con
tus
palabras
.
no
son
realmente
blogs
.
entonces
todo
esto
también
necesita
una
clasificación
de
este
tipo
de
páginas
.
con
ello
se
llega
a
los
objetivos
de
la
de
la
de
la
investigación
.
proponer
una
representación
general
de
las
páginas
para
clasificarlas
en
el
dominio
específico
y
proponer
una
representación
específica
para
el
tipo
de
páginas
blog
para
alcanzar
una
una
representación
mucho
más
eficiente
.
todo
ello
además
se
debe
de
trasladar
los
resultados
a
otros
dominios
para
validar
la
la
potencia
del
método
.
y
para
guiar
toda
esta
investigación
pues
se
fijan
unos
objetivos
secundarios
que
es
crear
una
colección
de
pruebas
sobre
la
que
realizar
los
experimentos
determinar
un
marco
de
evaluación
un
unas
necesidades
básicas
de
de
tratamiento
de
la
de
la
información
un
pretratamiento
en
todo
proceso
de
de
minería
y
fijar
un
marco
de
comparativo
una
baseline
sobre
la
que
poder
validar
nuestros
resultados
.
los
experimentos
de
clasificación
el
empezamos
con
la
con
la
creación
de
la
colección
de
pruebas
.
no
se
ha
utilizado
una
colección
de
pruebas
estándar
no
en
primer
lugar
no
había
ninguna
colección
de
pruebas
en
un
dominio
específico
como
éste
.
existen
muchas
genéricas
pero
en
dominio
específico
no
habría
y
se
realiza
un
crawl
.
se
obtienen
todas
las
páginas
que
nos
servirán
tanto
para
entrenamiento
como
para
validación
teniendo
un
conjunto
de
cuatro
mil
ochocientas
páginas
.
eh
la
disparidad
entre
entre
la
presencia
da
cada
una
de
las
categorías
es
bastante
grande
.
esto
afectará
a
los
resultados
pero
también
servirá
para
ver
la
la
fortaleza
del
método
cuando
los
entrenamientos
son
defectuosos
.
la
colección
de
pruebas
se
ha
dividido
pero
básicamente
las
páginas
de
un
mismo
sitio
en
cierto
modo
tienen
relación
entre
ellas
.
han
sido
creadas
seguramente
por
un
mismo
diseñador
hablan
más
o
menos
de
lo
mismo
va
a
dar
valores
más
altos
de
los
que
en
realidad
debería
de
dar
.
además
se
ha
obtenido
una
colección
de
pruebas
extendida
para
realizar
la
la
el
traspaso
de
del
método
fuera
del
dominio
del
teatro
.
para
ello
esto
sólo
ha
sido
para
las
páginas
de
tipo
blog
se
han
obtenido
tres
mil
seiscientas
noventa
y
seis
páginas
de
tipo
blog
clasificadas
manualmente
de
de
manera
previa
.
y
además
se
han
introducido
páginas
en
diferentes
idiomas
para
intentar
que
el
que
el
método
sea
independiente
tanto
del
contenido
como
del
idioma
.
y
se
ha
obtenido
un
crawl
de
del
directorio
de
Yahoo
para
obtener
las
las
páginas
de
de
alimentación
negativa
las
que
no
son
tipo
blog
obteniendo
en
total
cerca
de
de
nueve
mil
doscientas
páginas
.
la
necesidad
del
crawl
el
primer
experimento
pasarlo
rápidamente
.
el
marco
de
evaluación
es
importante
.
nos
hemos
basado
en
una
evaluación
típica
de
de
precisión
donde
los
el
entrenamiento
se
realiza
intentando
maximizar
el
número
de
aciertos
minimizar
el
número
de
errores
.
no
se
ha
introducido
coste
y
además
se
se
ha
aportado
relevancia
estadística
en
cuanto
a
intervalo
de
confianza
del
error
real
a
partir
del
error
muestral
que
hemos
cometido
de
manera
que
los
resultados
tengan
cierta
importancia
estadística
.
el
método
de
validación
que
se
ha
utilizado
ha
sido
uno
que
hemos
propuesto
específicamente
para
para
esto
.
es
el
el
denominado
dos
por
dos
el
problema
está
en
que
la
validación
cruzada
se
realiza
el
entrenamiento
moviendo
bloques
entre
los
datos
de
entrenamiento
y
los
de
validación
.
como
ya
decimos
estos
bloques
son
difíciles
porque
las
páginas
están
extraídas
de
un
mismo
sitio
y
tienen
bastante
relación
entre
ellas
.
por
lo
tanto
hemos
partido
lo
que
comentaba
la
colección
de
pruebas
en
dos
atendiendo
a
que
cada
parte
de
la
colección
de
pruebas
tuviera
páginas
de
un
sólo
sitio
.
y
se
ha
realizado
una
validación
cruzada
entre
ellas
primero
se
ha
entrenado
con
una
se
ha
validado
con
la
otra
se
ha
hecho
la
inversa
y
se
ha
sacado
la
media
un
método
como
el
de
Dietterich
cinco
por
dos
pero
con
menos
iteraciones
.
con
este
marco
de
evaluación
un
experimento
sencillo
para
ver
si
teníamos
razón
fue
éste
.
realizamos
una
validación
cruzada
y
obtuvimos
unos
resultados
esto
es
un
método
por
bolsa
de
palabras
estándar
sin
realizar
filtros
sin
realizar
pretratamiento
de
nada
.
y
con
una
validación
cruzada
hemos
obtenido
casos
hasta
del
noventa
por
ciento
de
un
estadístico
efe
.
eso
de
entrada
es
seguramente
es
falso
casi
todos
los
estudios
que
hay
están
en
el
treinta
al
cuarenta
por
cien
entonces
con
el
método
dos
por
dos
nos
da
algo
más
realista
el
treinta
el
cinco
el
diez
por
cien
.
a
las
necesidades
de
pro
procesamiento
están
claras
en
todos
los
proyectos
de
minería
.
en
éste
%eh
básicamente
son
tratamiento
del
corpus
y
tratamiento
lingüístico
.
una
aplicación
de
un
stem
sería
la
parte
más
sencilla
un
stem
de
Porter
y
una
selección
de
un
corpus
general
o
un
corpus
específico
.
los
experimentos
también
han
sido
claros
con
esto
.
el
no
es
necesario
para
aumentar
el
rendimiento
pero
sí
para
reducir
la
dimensionalidad
cosa
que
quizá
es
más
importante
aún
.
determinar
un
baseline
es
es
necesaria
.
con
todo
esto
ya
estaríamos
fijando
el
marco
de
trabajo
sobre
el
que
hacer
una
propuesta
y
poder
compararla
.
la
baseline
es
la
la
clásica
de
del
Bag
of
Words
la
bolsa
de
palabras
a
la
que
además
se
le
han
añadido
dos
mejoras
una
introduciendo
métodos
contextuales
como
el
título
las
las
URLs
o
los
o
los
elementos
enmarcados
y
otro
en
el
que
únicamente
se
obtienen
las
palabras
que
aparecen
en
la
URL
.
es
un
método
que
que
está
en
el
estado
del
arte
que
obtiene
resultados
muy
interesantes
y
que
los
hemos
querido
comparar
.
los
resultados
también
son
claros
ninguna
de
ellas
mejora
a
la
baseline
significativamente
a
nivel
de
de
significación
de
un
noventa
y
cinco
por
cien
incluso
.
pero
sí
que
se
encontró
que
que
el
método
de
las
URLs
en
algunos
casos
puntuales
daba
mucha
certeza
.
esto
es
fácil
de
entender
una
página
que
en
su
URL
tenga
la
palabra
festivales
seguramente
y
casi
cien
por
cien
de
seguridad
va
a
ser
de
la
categoría
festivales
.
una
página
que
tenga
blog
spot
va
a
ser
un
blog
a
partir
de
todo
ello
a
partir
de
estudios
que
que
realizamos
uno
de
los
métodos
que
más
nos
interesó
por
ser
de
los
que
mejores
resultados
conseguía
era
un
método
que
obtenía
una
clasificación
a
partir
de
un
resumen
de
la
página
web
.
el
resumen
se
puede
hacer
de
dos
maneras
o
lo
puede
hacer
una
persona
y
entonces
ya
no
estaríamos
en
este
proyecto
porque
ya
que
haces
el
resumen
haces
la
clasificación
.
o
utilizas
un
un
método
de
resumen
automático
por
la
semantic
analysis
o
algún
otro
método
que
son
bastante
complejos
y
es
otro
proyecto
de
investigación
.
eh
.
el
problema
de
la
cabecera
es
que
no
siempre
está
informada
el
problema
de
los
enlaces
hay
muchos
enlaces
del
tipo
pincha
aquí
entonces
por
sí
solos
no
no
ofrecen
información
combinándolos
una
ponderación
podría
fastidiar
a
la
otra
entonces
decidimos
triplicar
esta
característica
vale
.
los
resultados
son
son
bastante
claros
en
las
tablas
se
muestran
en
en
negrita
no
sé
si
se
ven
las
negritas
pero
vamos
la
mayoría
de
los
resultados
el
método
propuesto
que
es
el
de
la
derecha
tiene
mejores
mejoras
significativas
en
en
cuanto
a
la
prueba
efe
mejoras
de
hasta
setenta
puntos
en
en
este
estadístico
.
como
se
puede
ver
en
en
rojo
lo
que
acabo
de
comentar
.
este
sería
el
intervalo
de
confianza
con
otro
cálculo
pero
tenemos
lo
mismo
el
error
de
confianza
el
error
cometido
es
bastante
inferior
en
el
caso
de
del
método
que
que
se
propone
.
ahora
las
conclusiones
son
directas
se
ha
obtenido
una
una
mejora
significativa
pero
también
hay
que
tener
en
cuenta
los
problemas
la
sensibilidad
a
páginas
que
no
tengan
suficientemente
información
.
es
bastante
sensible
estos
casos
eh
.
le
cualquier
persona
que
vea
un
blog
nada
más
verlo
da
igual
en
qué
idioma
esté
qué
colores
tenga
quién
lo
haya
escrito
y
de
qué
sea
ese
blog
sabe
que
es
un
blog
.
eso
son
características
visuales
que
si
pudiéramos
plasmarlas
de
algún
modo
de
manera
formal
y
utilizarlas
para
clasificar
podríamos
obtener
bastante
eficiencia
y
ser
independientes
tanto
de
contenido
como
de
idioma
.
el
método
es
que
se
han
obtenido
quince
características
específicas
a
partir
de
la
estructura
HTML
de
los
blogs
.
y
básicamente
es
eso
tú
ves
un
blog
tiene
un
apartado
de
de
POST
es
un
diario
donde
el
usuario
va
escribiendo
POST
.
estos
POSTs
están
encabezados
con
el
título
del
POST
pueden
ser
etiquetas
H1
H2
diferentes
etiquetas
HTML
.
suele
tener
una
fecha
de
publicación
y
suele
tener
un
enlace
feedback
para
tener
retroalimentación
con
el
usuario
.
además
aparecen
un
conjunto
de
enlaces
siempre
juntos
que
se
donde
se
enlazan
a
otros
blogs
de
denominados
blogs
amigos
o
hacia
páginas
de
dentro
del
propio
archivo
de
del
blog
.
además
palabras
palabras
clave
o
palabras
reservadas
que
suelen
aparecer
bastante
en
el
contenido
son
la
palabra
POST
y
la
propia
palabra
blog
.
y
además
los
blogs
suelen
tener
suscripción
RSSATOM
.
no
es
exclusiva
de
ellos
no
la
tienen
todos
pero
es
un
otra
característica
que
da
certeza
.
todas
estas
características
se
combinan
mediante
ratios
obteniendo
las
quince
características
que
que
utilizamos
.
los
resultados
son
bastante
bastante
claros
.
en
la
prueba
efe
se
ha
obtenido
muy
por
encima
del
noventa
por
ciento
.
en
el
caso
de
de
que
no
sea
blog
una
página
casi
en
el
noventa
y
nueve
por
cien
de
los
casos
se
se
indica
así
.
comparándolo
con
la
baseline
son
bastante
elevados
.
la
estándar
está
alrededor
del
treinta
por
cien
el
nuestro
ha
sacado
el
noventa
y
dos
por
cien
la
que
propusimos
de
del
encabezado
sacó
un
sesenta
y
seis
por
cien
cuando
es
pertenencia
a
los
blogs
y
un
noventa
y
cuatro
cuando
no
lo
es
también
inferior
a
ésta
.
por
y
en
los
intervalos
de
error
lo
mismo
están
muy
por
debajo
del
dos
por
cien
el
error
real
con
un
un
intervalo
de
un
uno
por
cien
de
confianza
.
la
conclusiones
que
se
ha
obtenido
una
representación
novedosa
y
eficiente
y
que
supera
el
rendimiento
de
las
estudiadas
en
el
estado
del
arte
.
esta
representación
nos
interesaba
mucho
sacarla
fuera
del
del
del
ámbito
del
teatro
.
eh
.
los
resultados
también
son
bastante
claros
no
seguimos
manteniéndonos
por
encima
del
noventa
por
cien
en
en
el
estadístico
efe
.
%eh
el
error
se
mantiene
en
el
caso
de
que
sea
blog
incluso
se
disminuye
.
se
queda
por
debajo
del
del
uno
por
cien
está
en
el
doce
por
cien
de
error
.
ha
aparecido
un
pequeño
problema
en
la
matriz
de
contingencia
se
ve
hay
falsos
positivos
es
decir
hay
páginas
que
no
son
blogs
que
se
clasifican
como
blogs
aproximadamente
el
doce
por
ciento
de
ellas
.
y
estudiando
fueron
páginas
que
eran
grupos
de
noticias
con
estructuras
muy
muy
similares
a
los
blogs
.
entonces
las
conclusiones
son
claras
el
rendimiento
se
ha
mantenido
en
en
otros
dominios
se
ha
demostrado
que
se
que
se
que
es
independiente
del
contenido
de
los
blogs
.
da
igual
de
de
que
se
escriba
en
el
blog
incluso
es
independiente
del
idioma
en
el
que
esté
escrito
el
problema
que
aparece
es
que
se
incrementan
los
falsos
positivos
.
se
catalogan
noticias
como
blogs
una
recapitulación
de
experimentos
y
las
conclusiones
.
la
principal
conclusión
se
ha
fijado
el
marco
de
de
trabajo
con
la
colección
de
pruebas
la
evaluación
y
el
baseline
.
que
mejora
la
baseline
hasta
en
setenta
puntos
del
estadístico
efe
en
algunos
de
los
casos
en
entrenamientos
defectuosos
como
como
veíamos
.
pero
que
es
sensible
a
determinados
entrenamientos
y
validaciones
cuando
las
páginas
no
tienen
suficientemente
información
.
páginas
flash
que
no
tienen
enlaces
y
el
el
que
lo
creó
no
puso
nada
en
la
cabecera
.
y
encima
la
URL
es
muy
genérica
esas
páginas
no
no
suelen
tener
buenos
resultados
el
método
de
los
blogs
obtiene
un
estadístico
efe
por
encima
del
noventa
por
cien
mejora
significativamente
el
estado
del
arte
es
aplicable
a
cualquier
dominio
independiente
del
contenido
y
del
idioma
pero
presenta
ciertos
problemas
con
grupos
de
noticias
.
todo
esto
nos
da
lugar
a
líneas
de
investigación
futuras
y
a
posibles
aplicaciones
de
esto
.
en
cuanto
al
primer
método
habría
que
extenderlo
fuera
del
dominio
de
del
teatro
lo
mismo
que
se
ha
realizado
con
los
blogs
para
comprobar
que
se
adecua
a
otros
dominios
y
ver
su
su
veracidad
.
habría
que
introducir
alguna
nueva
característica
habría
que
investigar
para
estas
páginas
que
no
tienen
suficientemente
información
.
quizás
habría
que
meter
tratamiento
lingüístico
habría
que
probarlo
en
otros
idiomas
y
ver
si
es
necesario
realizar
stemmers
o
realizar
alguna
cosa
así
.
podría
ser
interesante
analizar
imágenes
desde
el
texto
alternativo
que
casi
nadie
pone
hasta
el
tamaño
de
las
imágenes
o
incluso
ya
meterse
en
histogramas
pero
bueno
eso
habría
que
investigarlo
.
los
los
clientes
lo
que
principalmente
nos
han
pedido
son
son
gerentes
de
clínicas
clínicas
privadas
pequeñas
de
ocho
o
diez
médicos
nos
quieren
controlar
a
los
trabajadores
porque
en
horas
de
trabajo
se
han
puesto
a
buscar
ofertas
de
trabajo
por
ejemplo
.
entonces
lo
primero
que
quieren
es
un
filtro
contra
búsqueda
de
empleo
.
estamos
trabajando
en
ello
el
tema
de
la
pederastia
es
un
tema
en
la
actualidad
bastante
peliagudo
bastante
jodido
.
estamos
dentro
de
un
dominio
específico
el
dominio
del
sexo
el
dominio
de
las
páginas
pornográficas
pero
con
páginas
que
que
están
buscando
un
tema
un
tema
ilegal
contacto
con
menores
.
la
aplicación
debería
ser
directa
pero
habría
que
investigar
a
ver
si
se
adapta
.
y
bueno
combinándolo
con
el
análisis
de
la
página
las
técnicas
de
antispam
están
haciendo
todo
lo
posible
porque
los
índices
de
los
buscadores
no
incluyan
cosas
como
lo
de
los
políticos
sudando
en
Sudán
y
cosas
así
no
.
si
se
analiza
todo
el
contenido
de
la
página
y
dice
que
la
página
va
de
una
cosa
y
el
autor
quiere
decir
en
su
metainformación
que
va
de
otra
cosa
ahí
se
está
dando
una
discordancia
.
es
posible
que
eso
sirva
eso
es
una
especulación
por
otro
lado
el
método
de
los
blogs
directamente
habría
que
mejorar
la
diferenciación
entre
blogs
y
noticias
para
que
obtuviera
resultados
mejores
.
pues
incorporarlo
en
algún
buscador
que
sea
un
buscador
real
de
blogs
que
los
contenidos
que
te
devuelva
realmente
sean
blogs
.
vale
con
todo
ello
terminaría
si
tienen
alguna
pregunta
